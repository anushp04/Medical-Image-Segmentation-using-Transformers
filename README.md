# Medical-Image-Segmentation-using-Transformers

Cancer is one of the top causes of death globally. Radiation therapy can be used to cure many variants of cancer. To administer large doses of radiation to malignancies while avoiding the internal organs, radiation oncologists use X-ray beams. Oncologists must physically delineate the position of the internal organs to focus the X-ray beam on the proper location, which is a very time-consuming and labor-intensive technique. This lengthens the process, which the patient may find uncomfortable. A technique for segmenting the internal organs might speed up treatments and enable more patients to receive more effective care. Labeling pixels in 2D images or voxels in 3D images is the process of segmentation. It is crucial for the quantification of delineated structures in medical imaging as well as the 3D representation of pertinent picture data. 

Traditionally, Fully Convolutional Neural Networks (FCNN), have been popularly used for model-based image segmentation. In FCNNs, the encoder plays a crucial role in learning global, local, and contextual representations that the decoder can use to anticipate semantic output. Despite its success, FCNNs' ability to learn long-range spatial relationships is constrained by the locality of their convolutional layers. We reframe the challenge of volumetric (3D) medical image segmentation as a sequence-to-sequence prediction problem, motivated by the recent success of transformers for Natural Language Processing (NLP) in long-range sequence learning. The Dataset that we will be using for this project consists of anonymized MRIs of patients treated at the UW-Madison Carbone Cancer Center. The training annotations are provided as RLE-encoded masks, and the images are in 16-bit grayscale PNG format. 

